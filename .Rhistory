library(scifetch)
z <- getgsauthor(id)
z
id <- "SPNX8oUAAAAJ"
z <- getgsauthor(id)
library(tidyRSS)
vignette("tidyrss", package = "tidyRSS")
tidyfeed("http://andrewgelman.com/feed/")
tidyfeed("http://andrewgelman.com/feed/")
tidyfeed("http://leiminnet.cn/blog/index.php/feed/")
install.packages("tidyRSS")
install.packages("tidyRSS")
install.packages("tidyRSS")
install.packages("tidyRSS")
tidyfeed("http://andrewgelman.com/feed/")
library(tidyRSS)
tidyfeed("http://andrewgelman.com/feed/")
tidyfeed("http://feeds.nature.com/nplants/rss/current")
library(scifetch)
?scifetch
getrss("http://feeds.nature.com/nplants/rss/current")
getrss("http://feeds.feedburner.com/ancham/")
tidyfeed("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
tidyfeed("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
feed.extract("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
library(feedeR)
feed.extract("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
feed <- xmlToList(feed$RDF)
xmlToList
??xmlToList
library(XML)
feed <- xmlToList("http://feeds.feedburner.com/oatmealfeed")
feed
title = feed$channel$title
title
link = feed$channel$description
link
link = feed$channel$items$Seq
link
feed <- xmlToList("http://leiminnet.cn/blog/index.php/feed/")
feed$channel$description
feed$channel$language
feed$channel$link
feed$channel$lastBuildDate
feed$channel$pubDate
feed$channel$link
feed$channel$item
feed$channel$item$description
feed$channel$item$comments
feed$channel$item$guid
feed$channel$item$link
feed$channel$item$guid
feed$channel$item$link
tidyfeed("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
feed <- xmlToList("http://leiminnet.cn/blog/index.php/feed/")
feed
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
a <- scifetch::getrss(m[1,1])
a <- scifetch::getrss(m[2,1])
a <- scifetch::getrss(m[3,1])
a <- scifetch::getrss(m[4,1])
a <- scifetch::getrss(m[5,1])
a <- scifetch::getrss(m[6,1])
a
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
n <- sum(a$date[1:NROW(a)])>as.Date(m[5,2]))
a$date[1:NROW(a)]
n <- sum(a$date[1:NROW(a)]>as.Date(m[5,2]))
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[5,2])-3)
n
a <- scifetch::getrss(m[5,1])
a
a
library(feedeR)
feed.extract("http://blogs.nature.com/naturejobs/feed/")
feed.extract("http://blogs.nature.com/feed/")
feed.extract("http://blogs.nature.com/feed/")
feed.extract("http://blogs.nature.com/naturejobs/feed/")
feed.extract("http://blogs.nature.com/blogs/feed/")
feed.extract("http://nature.com/blogs/feed/")
http://blogs.nature.com/naturejobs/feed/
feed.extract("https://www.nature.com/news/rss/all_index.rdf")
feed.extract("http://feeds.nature.com/news/rss/news_s11")
feed.extract("http://feeds.nature.com/news/rss/most_recent")
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
}
n
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
}
a
a$description
sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
feedeR::feed.extract("http://biodong.com/index.xml")
feedeR::feed.extract("http://biodong.com/index.xml", encoding = 'UTF-8')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
?rnorm
rnorm(1)
rnorm(10)
rnorm(100)
rnorm(1000)
rnorm(10000000)
for(ninx) {
if (n %% 2 == 1) k <- k+1
}
for(n in x) {
if (n %% 2 == 1) k <- k+1
}
mm = function (x) {}
mm = function (x) { for(n in x) {
if (n %% 2 == 1) k <- k+1
}}
k = 0
mm(c(1,2,3))
x <- c(5,12,13)
mode(x)
class(x)
str(x)
blogdown:::serve_site()
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
a <- scifetch::getrss(m[1,1])
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
a
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2])-12)
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2])-15)
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2])-25)
n
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
x
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
blogdown:::serve_site()
blogdown:::update_meta_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
library(feedeR)
feed.extract("https://projectreporter.nih.gov/_readers/new_RePORTER_projects.xml")
feed.extract("http://feeds.nature.com/nplants/rss/current")
install.packages('jsonlite')
res <- jsonlite::fromJSON("feeds.feedburner.com/StatsChat")
res <- jsonlite::fromJSON("feeds.feedburner.com/StatsChat")
library(jsonlite)
res <- jsonlite::fromJSON("feeds.feedburner.com/StatsChat")
res <- jsonlite::fromJSON("feeds.feedburner.com/StatsChat")
res <- fromJSON("feeds.feedburner.com/StatsChat")
res <- fromJSON("feeds.feedburner.com/StatsChat")
install.packages('rjson')
library(rjson)
res <- fromJSON("feeds.feedburner.com/StatsChat")
res <- jsonlite::fromJSON("feeds.feedburner.com/StatsChat")
data3 <- fromJSON("https://api.github.com/users/hadley/repos", flatten = TRUE)
identical(data3, flatten(data2))
data3 <- fromJSON("https://api.github.com/users/hadley/repos", flatten = TRUE)
data3 <- fromJSON("https://api.github.com/users/hadley/repos")
data3 <- fromJSON("api.github.com/users/hadley/repos")
a <- feed.extract("http://feeds.nature.com/nplants/rss/current")
a
thepage = readLines('http://calbears.cstv.com/sports/m-basebl/sched/cal-m-basebl-sched.html')
a <- readlines("http://feeds.nature.com/nplants/rss/current")
a <- readLines("http://feeds.nature.com/nplants/rss/current")
a
grep('Opponent / Event',thepage)
grep('Opponent / Event', a)
grep('content', a)
a <- feed.extract("http://feeds.nature.com/nplants/rss/current")
a
a$items
b = a$items
b$date
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
a <- feedeR::feed.extract(m[1,1])$items
m
m[1,1]
feed.extract(m[1,1])
feed.extract(m[2,1])
a <- feedeR::feed.extract(m[2,1])$items
a
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[2,2]))
n
as.POSIXct(m[2,2]))
as.POSIXct(m[2,2])
as.POSIXct(a$date[1:NROW(a)])
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- feedeR::feed.extract(m[i,1])$items
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
blogdown:::serve_site()
m
for (i in 1:NROW(m)) {
a <- feedeR::feed.extract(m[i,1])$items
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- feedeR::feed.extract(m[i,1])$items
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
m
for (i in 1:NROW(m)) {
a <- feedeR::feed.extract(m[i,1])$items
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
a <- feedeR::feed.extract(m[1,1])$items
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
n
a <- feedeR::feed.extract(m[2,1])$items
m[2,1]
a <- feedeR::feed.extract(m[2,1])$items
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
a <- feedeR::feed.extract(m[2,1])$items
a
for (i in 1:NROW(m)) {
a <- feedeR::feed.extract(m[i,1])$items
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- feedeR::feed.extract(m[i,1])$items
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- feedeR::feed.extract(m[i,1])$items
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
nrow(x)
p = sprintf('content/post/%s.md', paste0(d,'-',1))
p
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[1,]))
x[1, ]
x[1, ]
x
x
x
NROW(x)
n
scifetch::getrss(m[1,1])
scifetch::getrss(m[1,1])
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
scifetch::getrss(m[1,1])
scifetch::getrss("http://feeds.nature.com/nplants/rss/current")
aa = scifetch::getrss("http://feeds.nature.com/nplants/rss/current")
aa$title
aa$description
source('~/Dropbox/Github/papers/R/fetch.R')
