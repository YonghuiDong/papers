dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
a
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
x
n
a
NROW(a)
nrow(a)
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
n
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
as.POSIXct(m[1,2])
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2])) - 3
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
x
n
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]) - 3)
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
n
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]) - 3)
n
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
as.POSIXct(a$date[1:NROW(a)])
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2])-10)
n
as.POSIXct(m[1,2])
as.POSIXct(m[1,2])-3
as.POSIXct(m[1,2])-10
as.POSIXct(m[1,2])-10
as.POSIXct(m[1,2])
as.POSIXct(m[1,2]-10)
as.POSIXct(m[1,2])
as.POSIXct(m[1,2]) - 10
as.POSIXct(m[1,2])
as.POSIXct(m[1,2]) -1
as.POSIXct(m[1,2]) -2
as.POSIXct(m[1,2]) -3
as.POSIXct(m[1,2])
m[1,2]
m[1,2]-3
as.date(m[1,2])-3
as.Date(m[1,2])-3
a$date[1:NROW(a)]
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]))
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 3)
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 4)
n
as.Date(m[1,2]) - 4
as.Date(m[1,2]) - 10
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 5)
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 10)
temp <- a[1:n,]
x <- rbind(temp,x)
m[1,2] <- d
m
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 100)
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
blogdown:::serve_site()
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
m
a <- scifetch::getrss(m[1,1])
a
length(a)
nrow(a)
a <- scifetch::getrss(m[1,1], 20)
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
a$description
a$description[1]
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=5000)
}
a$description[1]
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
n
x
d
x
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
blogdown:::serve_site()
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
library(tidyRSS)
rss <- tidyfeed("feeds.feedburner.com/acs/esthag")
xml2::read_html("http://chinesenews.net.au", encoding = "BIG5")
install.packages("xml2")
install.packages("xml2")
library("xml2")
source('~/Dropbox/Github/papers/R/fetch.R')
xml2::read_html("http://chinesenews.net.au", encoding = "BIG5")
"? encoding"
? encoding
?encoding
??encoding
encoding = "latin1"
source('~/Dropbox/Github/papers/R/fetch.R')
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
install.packages('feedR')
install.packages('feedER')
install.packages('feedeR')
library(feedeR)
feed.extract("https://feeds.feedburner.com/RBloggers")
feed.extract("http://www.sciencemag.org/rss/news_current.xml")
feed.extract("http://leiminnet.cn/blog/index.php/feed/")
feed.extract("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
feed.extract("https://www.journals.elsevier.com/trends-in-analytical-chemistry/rss")
feed.extract("http://feeds.nature.com/nplants/rss/current")
library(scifetch)
m = "http://www.sciencemag.org/rss/news_current.xml"
a <- scifetch::getrss(m[1,1])
a <- scifetch::getrss("http://www.sciencemag.org/rss/news_current.xml")
a
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
m = m[1:3,]
m
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=2000)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
a <- scifetch::getrss(m[11,1])
a <- scifetch::getrss(m[1,1])
x
a <- scifetch::getrss(m[i,1])
m
a <- scifetch::getrss(m[1,1])
a
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[i,2])-100)
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[i,2])-10)
n
temp <- a[1:n,]
x <- rbind(temp, temp)
x
duplicated(x)
x[duplicated(x)]
x[!duplicated(x), ]
x <- rbind(temp, temp)
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=2000)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
x
n
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
m
f
m = "http://feeds.feedburner.com/acs/ancham/'
m = "http://feeds.feedburner.com/acs/ancham/"
a <- scifetch::getrss(m)
m = "https://yufree.cn/index.xml"
a <- scifetch::getrss(m)
a
m = "http://feeds.feedburner.com/ancham/"
a <- scifetch::getrss(m)
a
m = "https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss"
a <- scifetch::getrss(m)
a
m
a <- scifetch::getrss(m)
m = "https://www.journals.elsevier.com/trends-in-analytical-chemistry/rss"
a <- scifetch::getrss(m)
a
m = "http://feeds.nature.com/nplants/rss/current"
a <- scifetch::getrss(m)
a
m = "https://link.springer.com/search.rss?facet-content-type=Article&facet-journal-id=11306&channel-name=Metabolomics"
a <- scifetch::getrss(m)
a
a = "http://feeds.nature.com/nature/rss/current"
m = "http://feeds.nature.com/nature/rss/current"
a <- scifetch::getrss(m)
a
m = "http://www.sciencemag.org/rss/news_current.xml"
a <- scifetch::getrss(m)
a
source('~/Dropbox/Github/papers/R/fetch.R')
m = "http://feeds.feedburner.com/ancham/"
a <- scifetch::getrss(m)
a
m = "http://feeds.nature.com/nplants/rss/current"
a <- scifetch::getrss(m)
a
m = "https://link.springer.com/search.rss?facet-content-type=Article&facet-journal-id=11306&channel-name=Metabolomics"
a <- scifetch::getrss(m)
a
m = "http://feeds.nature.com/nature/rss/current"
a <- scifetch::getrss(m)
a
m = "http://www.sciencemag.org/rss/news_current.xml"
a <- scifetch::getrss(m)
a
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
a
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=2000)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=2000)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=2000)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
m = "https://link.springer.com/search.rss?facet-content-type=Article&facet-journal-id=11306&channel-name=Metabolomics"
a <- scifetch::getrss(m)
a
m = "yihui.name/index.xml","2018-03-21"
m = "yihui.name/index.xml"
a <- scifetch::getrss(m)
a
m = "https://link.springer.com/search.rss?facet-content-type=Article&facet-journal-id=11306&channel-name=Metabolomics"
a <- scifetch::getrss(m)
a$title
a$linkTitle
a$description
m = "https://yufree.cn/index.xml"
a <- scifetch::getrss(m)
a
