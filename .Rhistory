library(feedeR)
a <- feed.extract("http://feeds.nature.com/nplants/rss/current")
a <- feed.extract("http://feeds.nature.com/nplants/rss/current")
a <- feed.extract("http://feeds.nature.com/nplants/rss/current")
library(feedeR)
a <- feed.extract("yihui.name/index.xml")
feed.extract("https://feeds.feedburner.com/RBloggers")
library(feedeR)
feed.extract("https://feeds.feedburner.com/RBloggers")
devtools::install_github("DataWookie/feedeR")
devtools::install_github("DataWookie/feedeR")
devtools::install_github("DataWookie/feedeR")
library(feedeR)
feed.extract("https://feeds.feedburner.com/RBloggers")
feed.extract("yihui.name/index.xml")
a = feed.extract("yihui.name/index.xml")
a = feed.extract("https://yihui.name/index.xml")
a = feed.extract("https://yihui.name/index.xml")
a
a$items[]
a$items
a$title
a$link
a$updated
a$items$hash
a$items$link
a$items$title
a$items$link
Encoding(someX)<-'UTF-8'
Encoding(a)<-'UTF-8'
options(encoding='UTF-8')
a = feed.extract("https://yihui.name/index.xml")
a
a$items$title
a = feed.extract("https://yihui.name/index.xml", encoding = 'UTF-8')
a$items$title
feed <- xmlToList(feed$RDF)
parse.rdf(feed.read("http://feeds.feedburner.com/oatmealfeed"))
source('~/.active-rstudio-document')
parse.rdf(feed.read("http://feeds.feedburner.com/oatmealfeed"))
library(dplyr)
parse.rdf(feed.read("http://feeds.feedburner.com/oatmealfeed"))
?xmlToList
library(XML)
parse.rdf(feed.read("http://feeds.feedburner.com/oatmealfeed"))
?getURL
library(RCurl)
parse.rdf(feed.read("http://feeds.feedburner.com/oatmealfeed"))
?parse_date_time
library(lubridate)
parse.rdf(feed.read("http://feeds.feedburner.com/oatmealfeed"))
feed.read("http://feeds.feedburner.com/oatmealfeed")
a = feed.read("http://feeds.feedburner.com/oatmealfeed")
a
a$comment
a$RDF$channel
a$RDF$item
a = parse.rdf(feed.read("http://feeds.feedburner.com/oatmealfeed"))
a$title
a = feed.read("http://feeds.feedburner.com/oatmealfeed")
a
source('~/.active-rstudio-document')
a = parse.rdf(feed.read("http://feeds.feedburner.com/oatmealfeed"))
a
a = parse.rdf(feed.read("http://feeds.feedburner.com/oatmealfeed"))
a$title
a$items
a$items$decription
source('~/.active-rstudio-document')
a = feed.read("http://feeds.feedburner.com/oatmealfeed")
a$`xml-stylesheet`
a$comment
a$RDF$item
a$RDF$channel
a <- feed.extract("http://feeds.nature.com/nplants/rss/current")
library(feedeR)
a <- feed.extract("http://feeds.nature.com/nplants/rss/current")
library(feedeR)
a <- feed.extract("http://feeds.nature.com/nplants/rss/current")
a
a$items$title
a$items$link
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
m = m[7]
m = m
m
m[1]
m[1,]
m[7,]
m = m[7,]
a <- feed.extract(m[i,1])
a <- feed.extract(m[1,1])
a
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
a
a <- scifetch::getrss(m[1,1])
a
m
a <- scifetch::getrss(m[1,1])
m = m[1,]
a <- scifetch::getrss(m[1,1])
a
library(scifetch)
a <- scifetch::getrss(m[1,1])
a
m = m[3,]
a
m
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
mm = m[1,]
mm
a <- scifetch::getrss(mm[1,1])
a
b <- feed.extract(mm[1,1])
b
b = b$items
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(mm[1,2]))
n
n2 <- sum(as.POSIXct(a$date[1:NROW(b)])>as.POSIXct(mm[1,2]))
n2
n2
n2 <- sum(as.POSIXct(a$date[1:NROW(b)])>as.Date(mm[1,2])-10)
n2
n2 <- sum(as.POSIXct(b$date[1:NROW(b)])>as.Date(mm[1,2])-10)
n2
b$date
a$date
a$date[1:nrow(a)]
n2 <- sum(as.POSIXct(b$date[1:NROW(b)])>as.Date(mm[1,2])-10)
n2
n2 <- sum(as.POSIXct(b$date[1:NROW(b)])>as.Date(mm[1,2])-100)
n2
if(n2>0){
temp <- b[1:n,]
x <- rbind(temp,x)
## update date
mm[1,2] <- d
}
1/0.065
1000/400
1034/400*46000
1000/400*46000
9.65/0.73
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
m
a <- scifetch::getrss(m[1,1])
a
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[1,2] <- d
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
n = 3
d = Sys.Date()
p2 = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
z = as.Date(gsub('-\\d{1,}.md$', '', p2)) < (d-n)
m = length(z[z==TRUE])
file.remove(file.path('content/post/', p2[1:m]))
blogdown:::serve_site()
blogdown:::serve_site()
m
m
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
# only keep the recent n-day (i.e. n = 3) markdown files in post directory
n = 3
d = Sys.Date()
p2 = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
z = as.Date(gsub('-\\d{1,}.md$', '', p2)) < (d-n)
m = length(z[z==TRUE])
file.remove(file.path('content/post/', p2[1:m]))
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
# only keep the recent n-day (i.e. n = 3) markdown files in post directory
n = 3
d = Sys.Date()
p2 = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
z = as.Date(gsub('-\\d{1,}.md$', '', p2)) < (d-n)
m = length(z[z==TRUE])
file.remove(file.path('content/post/', p2[1:m]))
blogdown:::serve_site()
blogdown:::serve_site()
