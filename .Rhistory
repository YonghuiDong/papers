sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
library(scifetch)
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
a <- scifetch::getrss(m[2,1])
a <- scifetch::getrss(m[1,1])
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
a <- scifetch::getrss(m[i,1])
a <- scifetch::getrss(m[1,1])
a
a <- scifetch::getrss(m[2,1])
a <- scifetch::getrss(m[3,1])
a <- scifetch::getrss(m[4,1])
source('~/Dropbox/Github/papers/R/fetch.R')
m
?getrss
feed <- 'yihui.name/index.xml'
z <- getrss(feed)
feed
z <- getrss(feed)
install.packages('tidyRSS')
library(tidyRSS)
rss <- tidyfeed("http://andrewgelman.com/feed/")
library(tidyRSS)
rss <- tidyfeed("feeds.feedburner.com/acs/estlcu")
library(tidyRSS)
rss <- tidyfeed("feeds.feedburner.com/acs/estlcu")
library(tidyRSS)
rss <- tidyfeed("www.journals.elsevier.com/water-research/rss")
rss
library(tidyRSS)
rss <- tidyfeed("yihui.name/index.xml")
rss
library(tidyRSS)
rss <- tidyfeed("biodong.com/index.xml")
rss
rss$feed_title
rss$feed_language
rss$feed_link
library(tidyRSS)
rss <- tidyfeed("www.journals.elsevier.com/water-research/rss")
rss
rss$feed_link
rss$item_link]
rss$item_link
library(tidyRSS)
rss <- tidyfeed("biodong.com/index.xml")
rss$item_link
library(tidyRSS)
rss <- tidyfeed("feeds.feedburner.com/acs/ancham/")
library(tidyRSS)
rss <- tidyfeed("https://hayobethlehem.nl/feed/atom/")
library(tidyRSS)
rss <- tidyfeed("https://hayobethlehem.nl/feed/atom/", encoding = "latin1")
?tidyfeed
tidyfeed("http://fivethirtyeight.com/all/feed")
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
library(scifetch)
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
a <- scifetch::getrss(m[1,1])
a <- scifetch::getrss(m[2,1])
a <- scifetch::getrss(m[3,1])
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
a <- scifetch::getrss(m[1,1])
a
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
a$date[1:NROW(a)])
a$date[1:NROW(a)]
as.POSIXct(m[i,2]
)
as.POSIXct(m[i,2])-3
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
a <- scifetch::getrss(m[1,1])
a
a <- scifetch::getrss(m[2,1])
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
a <- scifetch::getrss(m[1,1])
a <- scifetch::getrss(m[3,1])
m
a <- scifetch::getrss(m[2,1])
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
a <- scifetch::getrss(m[i,1])
a <- scifetch::getrss(m[1,1])
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
a
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
n
a$date[1:NROW(a)]
a
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
source('~/Dropbox/Github/papers/R/fetch.R')
a <- scifetch::getrss(m[1,1])
a
a <- scifetch::getrss(m[2,1])
a <- scifetch::getrss(m[3,1])
a <- scifetch::getrss(m[3,1])
feed <- 'yihui.name/index.xml'
z <- getrss(feed)
z
feed <- 'biodong.com/index.xml'
z <- getrss(feed)
z
feed <- 'yihui.name/index.xml'
z <- getrss(feed)
z
source('~/Dropbox/Github/papers/R/fetch.R')
feed <- "www.journals.elsevier.com/water-research/rss"
z <- getrss(feed)
z
feed <- "feeds.feedburner.com/acs/estlcu"
z <- getrss(feed)
res <- jsonlite::fromJSON(feed)
feed
feed <- 'yihui.name/index.xml'
res <- jsonlite::fromJSON(feed)
resetClass()
z <- getrss(feed)
feed <- 'http://biodong.com/index.xml'
z <- getrss(feed)
z
feed <- 'biodong.com/index.xml'
feed
z <- getrss(feed)
feed <- 'http://leiminnet.cn/blog/index.php/feed/'
z <- getrss(feed)
z
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
a
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
x
n
a
NROW(a)
nrow(a)
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
n
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
as.POSIXct(m[1,2])
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2])) - 3
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
x
n
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]) - 3)
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
n
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]) - 3)
n
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
as.POSIXct(a$date[1:NROW(a)])
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2])-10)
n
as.POSIXct(m[1,2])
as.POSIXct(m[1,2])-3
as.POSIXct(m[1,2])-10
as.POSIXct(m[1,2])-10
as.POSIXct(m[1,2])
as.POSIXct(m[1,2]-10)
as.POSIXct(m[1,2])
as.POSIXct(m[1,2]) - 10
as.POSIXct(m[1,2])
as.POSIXct(m[1,2]) -1
as.POSIXct(m[1,2]) -2
as.POSIXct(m[1,2]) -3
as.POSIXct(m[1,2])
m[1,2]
m[1,2]-3
as.date(m[1,2])-3
as.Date(m[1,2])-3
a$date[1:NROW(a)]
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]))
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 3)
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 4)
n
as.Date(m[1,2]) - 4
as.Date(m[1,2]) - 10
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 5)
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 10)
temp <- a[1:n,]
x <- rbind(temp,x)
m[1,2] <- d
m
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2]) - 100)
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
blogdown:::serve_site()
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
m
a <- scifetch::getrss(m[1,1])
a
length(a)
nrow(a)
a <- scifetch::getrss(m[1,1], 20)
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
a$description
a$description[1]
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=5000)
}
a$description[1]
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[1,2]))
n
x
d
x
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
blogdown:::serve_site()
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
library(tidyRSS)
rss <- tidyfeed("feeds.feedburner.com/acs/esthag")
xml2::read_html("http://chinesenews.net.au", encoding = "BIG5")
install.packages("xml2")
install.packages("xml2")
library("xml2")
source('~/Dropbox/Github/papers/R/fetch.R')
xml2::read_html("http://chinesenews.net.au", encoding = "BIG5")
"? encoding"
? encoding
?encoding
??encoding
encoding = "latin1"
source('~/Dropbox/Github/papers/R/fetch.R')
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
