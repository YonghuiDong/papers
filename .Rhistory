x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
m
a <- scifetch::getrss(m[1,1])
a
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[1,2] <- d
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
n = 3
d = Sys.Date()
p2 = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
z = as.Date(gsub('-\\d{1,}.md$', '', p2)) < (d-n)
m = length(z[z==TRUE])
file.remove(file.path('content/post/', p2[1:m]))
blogdown:::serve_site()
blogdown:::serve_site()
m
m
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
# only keep the recent n-day (i.e. n = 3) markdown files in post directory
n = 3
d = Sys.Date()
p2 = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
z = as.Date(gsub('-\\d{1,}.md$', '', p2)) < (d-n)
m = length(z[z==TRUE])
file.remove(file.path('content/post/', p2[1:m]))
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
# only keep the recent n-day (i.e. n = 3) markdown files in post directory
n = 3
d = Sys.Date()
p2 = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
z = as.Date(gsub('-\\d{1,}.md$', '', p2)) < (d-n)
m = length(z[z==TRUE])
file.remove(file.path('content/post/', p2[1:m]))
blogdown:::serve_site()
blogdown:::serve_site()
library(MSbox)
mz('C50H83NO21h1', 1)
mz('C50H83N1O21h1', 1)
install.packages("webchem")
library("webchem")
cir_query('Triclosan', 'cas')
cir_query('triclosan', 'cas')
cir_query('triclosAn', 'cas')
cir_query('Triclosan', 'mw')
cir_query('TOMATINE', 'mw')
?cir_query
cir_query('TOMATINE', 'image')
cir_query('TOMATINE', 'hashisy')
cir_query('Aldehyde', 'mw')
cir_query('Aldehyde', 'mZ')
cir_query('Aldehyde', 'mz')
cir_query('Aldehyde', 'm/z')
cir_query('Aldehyde', '')
cir_query('Aldehyde')
cir_query('TOMATINE')
cir_query('nitrogen')
cir_query('hi')
cir_query('pi')
ppm(1034.5534, .5553)
ppm(1034.5534, .5530)
ppm(1034.55335, .55304)
iso_mz('C48H82N1O21', iso = '[13]C2)
iso_mz('C48H82N1O21', iso = '[13]C2')
Iso_mz('C48H82N1O21', iso = '[13]C2', z = 1)
Iso_mz('C50H82N1O21', iso = '[13]C2', z = 1)
1034.5530 / (1034.5441 - 1034.5530)
library(feedeR)
rbloggers <- feed.extract("https://feeds.feedburner.com/RBloggers")
a <- feed.extract("https://feeds.feedburner.com/RBloggers")
nrow(a$items)
a$items
library(xml2)
read_html(a$items$link)
read_html(a$items$link[1])
x = read_html(a$items$link[1])
x$doc
x[2]
x$node
x
read_html("<html><title>Hi")
a
a$items$date
a$items$title
a$items$link
a$items$link[1]
b = a$items$link[1]
blogdown:::serve_site()
install.packages('jsonlite')
install.packages("jsonlite")
library(jsonlite)
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
n = m[1]
n
n = m[1,]
n
n = m[1,1]
n
res <- jsonlite::fromJSON(feed)
res <- jsonlite::fromJSON(n)
formats <- c("a d b Y H:M:S z", "a, d b Y H:M z",
"Y-m-d H:M:S z", "d b Y H:M:S",
"d b Y H:M:S z", "a b d H:M:S z Y",
"a b dH:M:S Y")
res <- jsonlite::fromJSON(n)
res
res <- fromJSON(n)
?fromJSON
data1 <- fromJSON("https://api.github.com/users/hadley/orgs")
names(data1)
n
n = m[2,1]
n
res <- fromJSON(n)
data1 <- fromJSON("https://api.github.com/users/hadley/orgs")
names(data1)
items <- data1$items
items
id <- "SPNX8oUAAAAJ"
z <- getgsauthor(id)
library(scifetch)
z <- getgsauthor(id)
z
id <- "SPNX8oUAAAAJ"
z <- getgsauthor(id)
library(tidyRSS)
vignette("tidyrss", package = "tidyRSS")
tidyfeed("http://andrewgelman.com/feed/")
tidyfeed("http://andrewgelman.com/feed/")
tidyfeed("http://leiminnet.cn/blog/index.php/feed/")
install.packages("tidyRSS")
install.packages("tidyRSS")
install.packages("tidyRSS")
install.packages("tidyRSS")
tidyfeed("http://andrewgelman.com/feed/")
library(tidyRSS)
tidyfeed("http://andrewgelman.com/feed/")
tidyfeed("http://feeds.nature.com/nplants/rss/current")
library(scifetch)
?scifetch
getrss("http://feeds.nature.com/nplants/rss/current")
getrss("http://feeds.feedburner.com/ancham/")
tidyfeed("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
tidyfeed("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
feed.extract("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
library(feedeR)
feed.extract("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
feed <- xmlToList(feed$RDF)
xmlToList
??xmlToList
library(XML)
feed <- xmlToList("http://feeds.feedburner.com/oatmealfeed")
feed
title = feed$channel$title
title
link = feed$channel$description
link
link = feed$channel$items$Seq
link
feed <- xmlToList("http://leiminnet.cn/blog/index.php/feed/")
feed$channel$description
feed$channel$language
feed$channel$link
feed$channel$lastBuildDate
feed$channel$pubDate
feed$channel$link
feed$channel$item
feed$channel$item$description
feed$channel$item$comments
feed$channel$item$guid
feed$channel$item$link
feed$channel$item$guid
feed$channel$item$link
tidyfeed("https://onlinelibrary.wiley.com/action/showFeed?jc=10982787&type=etoc&feed=rss")
feed <- xmlToList("http://leiminnet.cn/blog/index.php/feed/")
feed
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
a <- scifetch::getrss(m[1,1])
a <- scifetch::getrss(m[2,1])
a <- scifetch::getrss(m[3,1])
a <- scifetch::getrss(m[4,1])
a <- scifetch::getrss(m[5,1])
a <- scifetch::getrss(m[6,1])
a
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
n <- sum(a$date[1:NROW(a)])>as.Date(m[5,2]))
a$date[1:NROW(a)]
n <- sum(a$date[1:NROW(a)]>as.Date(m[5,2]))
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[5,2])-3)
n
a <- scifetch::getrss(m[5,1])
a
a
library(feedeR)
feed.extract("http://blogs.nature.com/naturejobs/feed/")
feed.extract("http://blogs.nature.com/feed/")
feed.extract("http://blogs.nature.com/feed/")
feed.extract("http://blogs.nature.com/naturejobs/feed/")
feed.extract("http://blogs.nature.com/blogs/feed/")
feed.extract("http://nature.com/blogs/feed/")
http://blogs.nature.com/naturejobs/feed/
feed.extract("https://www.nature.com/news/rss/all_index.rdf")
feed.extract("http://feeds.nature.com/news/rss/news_s11")
feed.extract("http://feeds.nature.com/news/rss/most_recent")
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
}
n
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
}
a
a$description
sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
feedeR::feed.extract("http://biodong.com/index.xml")
feedeR::feed.extract("http://biodong.com/index.xml", encoding = 'UTF-8')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
?rnorm
rnorm(1)
rnorm(10)
rnorm(100)
rnorm(1000)
rnorm(10000000)
for(ninx) {
if (n %% 2 == 1) k <- k+1
}
for(n in x) {
if (n %% 2 == 1) k <- k+1
}
mm = function (x) {}
mm = function (x) { for(n in x) {
if (n %% 2 == 1) k <- k+1
}}
k = 0
mm(c(1,2,3))
x <- c(5,12,13)
mode(x)
class(x)
str(x)
blogdown:::serve_site()
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
m
a <- scifetch::getrss(m[1,1])
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
a
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2])-12)
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2])-15)
n
n <- sum(a$date[1:NROW(a)]>as.Date(m[1,2])-25)
n
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
x
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
blogdown:::serve_site()
blogdown:::update_meta_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
