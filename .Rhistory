blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
23-17
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
n = 3
d = Sys.Date()
p2 = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
z = as.Date(gsub('-\\d{1,}.md$', '', p2)) < (d-n)
m = length(z[z==TRUE])
file.remove(file.path('content/post/', p2[1:m]))
install_github('yufree/scifetch')
library(devtools)
install_github('yufree/scifetch')
library(scifetch)
source('~/Dropbox/Github/papers/R/fetch.R', echo=TRUE)
getrss <- function(feed){
# This function is modified from tidyRSS and credits should belong to the author of tidyRSS package
formats <- c("a d b Y H:M:S z", "a, d b Y H:M z",
"Y-m-d H:M:S z", "d b Y H:M:S",
"d b Y H:M:S z", "a b d H:M:S z Y",
"a b dH:M:S Y")
json_parse <- function(feed){
res <- jsonlite::fromJSON(feed)
items <- res$items
results <- tibble::tibble(
title = items$title,
date = lubridate::parse_date_time(items$date_published, orders = formats) %>%
as.character() %>% substr(1,10),
linkTitle = items$url,
source = res$title,
description = items$content_html
)
return(results)
}
rss_parse <- function(doc){
channel <- xml2::xml_find_all(doc, "channel")
if(identical(length(channel), 0L)){
ns <- xml2::xml_ns_rename(xml2::xml_ns(doc), d1 = "rss")
channel <- xml2::xml_find_all(doc, "rss:channel", ns = ns)
site <- xml2::xml_find_all(doc, "rss:item", ns = ns)
res <- suppressWarnings({tibble::tibble(
title = xml2::xml_text(xml2::xml_find_all(site, "rss:title", ns = ns)),
date = xml2::xml_text(xml2::xml_find_first(site, "rss:pubDate", ns = ns)) %>%
lubridate::parse_date_time(orders = formats) %>%
as.character() %>% substr(1,10),
linkTitle = xml2::xml_text(xml2::xml_find_all(site, "rss:link", ns = ns)),
source = xml2::xml_text(xml2::xml_find_first(channel, "rss:title", ns = ns)),
description = xml2::xml_text(xml2::xml_find_first(site, "rss:description", ns = ns))
)})
} else{
site <- xml2::xml_find_all(channel, "item")
res <- suppressWarnings({tibble::tibble(
title = xml2::xml_text(xml2::xml_find_first(site, "title")),
date = xml2::xml_text(xml2::xml_find_first(site, "pubDate")) %>%
lubridate::parse_date_time(orders = formats) %>%
as.character() %>% substr(1,10),
linkTitle = xml2::xml_text(xml2::xml_find_first(site, "link")),
source = xml2::xml_text(xml2::xml_find_first(channel, "title")),
description = xml2::xml_text(xml2::xml_find_first(site, "description"))
)})
res <- Filter(function(x) !all(is.na(x)), res)
return(res)
}
}
atom_parse <- function(doc){
channel <- xml2::xml_find_all(doc, "channel", ns = xml2::xml_ns(doc))
site <- xml2::xml_find_all(channel, "item")
res <- tibble::tibble(
title = xml2::xml_text(xml2::xml_find_first(site, ns = xml2::xml_ns(doc), "title")),
date = xml2::xml_text(xml2::xml_find_first(site, ns = xml2::xml_ns(doc),
"pubDate")) %>%
lubridate::parse_date_time(orders = formats) %>%
as.character() %>% substr(1,10),
linkTitle = xml2::xml_text(xml2::xml_find_first(site, ns = xml2::xml_ns(doc),
"link")),
source = xml2::xml_text(xml2::xml_find_first(channel, ns = xml2::xml_ns(doc), "title")),
description = xml2::xml_text(xml2::xml_find_all(site, ns = xml2::xml_ns(doc), "description")))
return(res)
}
invisible({
suppressWarnings({
stopifnot(identical(length(feed), 1L)) # exit if more than 1 feed provided
msg <- "Error in feed parse; please check URL."
doc <- try(httr::GET(feed), silent = TRUE)
if(grepl("json", doc)){
result <- json_parse(feed)
return(result)
} else{
doc <- doc %>% xml2::read_xml()
}
if(unique(grepl('try-error', class(doc)))){
stop(msg)
}
if(grepl("http://www.w3.org/2005/Atom", doc)){
result <- atom_parse(doc)
return(result)
} else{
result <- rss_parse(doc)
return(result)
}
})
})
}
for (i in 1:NROW(m)) {
a <- getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
source('~/Dropbox/Github/papers/R/fetch.R', echo=TRUE)
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
m
m[1, 1]
m[2, 1]
m[2, 2]
m[2, 1]
a <- scifetch::getrss(m[1,1])
a
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
a <- scifetch::getrss(m[15,1])
a <- scifetch::getrss(m[14,1])
a
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
n
temp <- a[1:n,]
temp
x <- rbind(temp,x)
x
m[14,2] <- d
x
NROW(x)>0
if(NROW(x)>0){
for (i in 1:NROW(x)){
p = sprintf('content/post/%s.md', paste0(d,'-',i))
sink(p)
cat('---\n')
cat(yaml::as.yaml(x[i,]))
cat('---\n')
sink()
}
}
write.csv(m[order(m$update), , drop = FALSE], f, row.names = FALSE)
source('~/Dropbox/Github/papers/R/fetch.R', echo=TRUE)
source('~/Dropbox/Github/papers/R/fetch.R', echo=TRUE)
source('~/Dropbox/Github/papers/R/fetch.R', echo=TRUE)
source('~/Dropbox/Github/papers/R/fetch.R')
dir.create('content/post', showWarnings = FALSE)
d = Sys.Date()
p = list.files('content/post/', '^\\d{4,}-\\d{2}-\\d{2}-\\d{1,}[.]md$')
p = max(as.Date(gsub('-\\d{1,}.md$', '', p)))
if (length(p) && d <= p && !interactive()) q('no')
if (!file.exists(f <- 'R/list.txt')) writeLines('website, update', f)
m = read.csv(f, colClasses = "character")
d = as.character(d)
x = NULL
n = 0
for (i in 1:NROW(m)) {
a <- scifetch::getrss(m[i,1])
# control the abs length
if(NROW(a)>0){
a$description <- substr(a$description,start=1, stop=500)
}
n <- sum(as.POSIXct(a$date[1:NROW(a)])>as.POSIXct(m[i,2]))
if(n>0){
temp <- a[1:n,]
x <- rbind(temp,x)
## update date
m[i,2] <- d
}
}
library(scifetch)
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
source('~/Dropbox/Github/papers/R/fetch.R')
